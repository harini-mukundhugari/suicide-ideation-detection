{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe133757-fb8f-4077-a4cb-885df729123c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet                  Suicide\n",
      "0                                  making some lunch         Not Suicide post\n",
      "1                        @Alexia You want his money.         Not Suicide post\n",
      "2  @dizzyhrvy that crap took me forever to put to...  Potential Suicide post \n",
      "3  @jnaylor #kiwitweets Hey Jer! Since when did y...         Not Suicide post\n",
      "4  Trying out &quot;Delicious Library 2&quot; wit...         Not Suicide post\n",
      "Missing values:\n",
      "Tweet    2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('Suicide_Ideation_Dataset(Twitter-based).csv')\n",
    "print(df.head())\n",
    "missing_values=df.isnull().sum()\n",
    "#to check missing values in specific column then we use\n",
    "#miss_values=df['col_name'].isnull().sum()\n",
    "print(\"Missing values:\")\n",
    "print(missing_values[missing_values>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c665772-f3ed-45db-8c1f-a1af230f34f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "target_column='Tweet'\n",
    "imputer=SimpleImputer(strategy='most_frequent') #for numerical mean and median recommended median. For categorical or nominal : most_frequent.\n",
    "df=pd.read_csv('Suicide_Ideation_Dataset(Twitter-based).csv')\n",
    "df[[target_column]]=imputer.fit_transform(df[[target_column]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fbea42a-dff6-4c58-96c3-1ca76f24c493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tweet      0\n",
       "Suicide    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dfeb2dc-a04f-41ac-81d0-55cda1c249e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before conversion:\n",
      "                                               Tweet                  Suicide\n",
      "0                                  making some lunch         Not Suicide post\n",
      "1                        @Alexia You want his money.         Not Suicide post\n",
      "2  @dizzyhrvy that crap took me forever to put to...  Potential Suicide post \n",
      "3  @jnaylor #kiwitweets Hey Jer! Since when did y...         Not Suicide post\n",
      "4  Trying out &quot;Delicious Library 2&quot; wit...         Not Suicide post\n",
      "\n",
      "After conversion:\n",
      "                                               Tweet  Suicide\n",
      "0                                  making some lunch      0.0\n",
      "1                        @Alexia You want his money.      0.0\n",
      "2  @dizzyhrvy that crap took me forever to put to...      NaN\n",
      "3  @jnaylor #kiwitweets Hey Jer! Since when did y...      0.0\n",
      "4  Trying out &quot;Delicious Library 2&quot; wit...      0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Suicide_Ideation_Dataset(Twitter-based).csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Before conversion:\")\n",
    "print(df.head())\n",
    "\n",
    "# Assume the column to convert is 'string_column'\n",
    "column_to_convert = 'Suicide'\n",
    "\n",
    "# Create a mapping dictionary\n",
    "# Assume the column contains 'yes' and 'no' as values\n",
    "binary_mapping = {'Potential Suicide post': 1, 'Not Suicide post': 0}\n",
    "\n",
    "# Apply the mapping to the column\n",
    "df[column_to_convert] = df[column_to_convert].map(binary_mapping)\n",
    "\n",
    "# Display the first few rows of the dataset after conversion\n",
    "print(\"\\nAfter conversion:\")\n",
    "print(df.head())\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d9199e9-c254-45da-92f9-5adb278d2a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detecting NaN values:\n",
      "0       False\n",
      "1       False\n",
      "2        True\n",
      "3       False\n",
      "4       False\n",
      "        ...  \n",
      "1782    False\n",
      "1783    False\n",
      "1784    False\n",
      "1785    False\n",
      "1786    False\n",
      "Name: Suicide, Length: 1787, dtype: bool\n",
      "\n",
      "DataFrame after filling NaN with 0:\n",
      "                                                  Tweet  Suicide\n",
      "0                                     making some lunch      0.0\n",
      "1                           @Alexia You want his money.      0.0\n",
      "2     @dizzyhrvy that crap took me forever to put to...      1.0\n",
      "3     @jnaylor #kiwitweets Hey Jer! Since when did y...      0.0\n",
      "4     Trying out &quot;Delicious Library 2&quot; wit...      0.0\n",
      "...                                                 ...      ...\n",
      "1782    i have forgotten how much i love my Nokia N95-1      0.0\n",
      "1783  Starting my day out with a positive attitude! ...      0.0\n",
      "1784  @belledame222 Hey, it's 5 am...give a girl som...      0.0\n",
      "1785  2 drunken besties stumble into my room and we ...      0.0\n",
      "1786  @dancingbonita &quot;I friggin love you!!!&quo...      0.0\n",
      "\n",
      "[1787 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDetecting NaN values:\")\n",
    "print(pd.isna(df['Suicide']))\n",
    "\n",
    "# Handling NaN values: Filling NaN with a specific value\n",
    "df['Suicide'] = df['Suicide'].fillna(1)\n",
    "print(\"\\nDataFrame after filling NaN with 0:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a675225-612c-4fc6-924e-8a6e8505bc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Tweet  Suicide\n",
      "0                                     making some lunch      0.0\n",
      "1                           @Alexia You want his money.      0.0\n",
      "2     @dizzyhrvy that crap took me forever to put to...      1.0\n",
      "3     @jnaylor #kiwitweets Hey Jer! Since when did y...      0.0\n",
      "4     Trying out &quot;Delicious Library 2&quot; wit...      0.0\n",
      "...                                                 ...      ...\n",
      "1780    i have forgotten how much i love my Nokia N95-1      0.0\n",
      "1781  Starting my day out with a positive attitude! ...      0.0\n",
      "1782  @belledame222 Hey, it's 5 am...give a girl som...      0.0\n",
      "1783  2 drunken besties stumble into my room and we ...      0.0\n",
      "1784  @dancingbonita &quot;I friggin love you!!!&quo...      0.0\n",
      "\n",
      "[1785 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "inconsistent_rows = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    tweet = row['Tweet']\n",
    "\n",
    "    # Check for missing values or special characters\n",
    "    if pd.isnull(tweet) or not isinstance(tweet, str) or tweet.strip() == '':\n",
    "        inconsistent_rows.append(index)\n",
    "\n",
    "# Remove inconsistent rows\n",
    "df = df.drop(inconsistent_rows)\n",
    "\n",
    "# Reset index if needed\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the clean data to a new CSV file\n",
    "\n",
    "\n",
    "# Display the cleaned dataset\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc40c165-dde2-4df7-a1a5-5ce1d24df92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Tweet  Suicide\n",
      "0                                     making some lunch      0.0\n",
      "1                           @Alexia You want his money.      0.0\n",
      "2     @dizzyhrvy that crap took me forever to put to...      1.0\n",
      "3     @jnaylor #kiwitweets Hey Jer! Since when did y...      0.0\n",
      "4     Trying out &quot;Delicious Library 2&quot; wit...      0.0\n",
      "...                                                 ...      ...\n",
      "1780    i have forgotten how much i love my Nokia N95-1      0.0\n",
      "1781  Starting my day out with a positive attitude! ...      0.0\n",
      "1782  @belledame222 Hey, it's 5 am...give a girl som...      0.0\n",
      "1783  2 drunken besties stumble into my room and we ...      0.0\n",
      "1784  @dancingbonita &quot;I friggin love you!!!&quo...      0.0\n",
      "\n",
      "[1777 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df.duplicated().sum()\n",
    "df = df.drop_duplicates()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da142cdd-f624-4ac8-97b7-61b458f19934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sridher\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sridher\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet  Suicide  \\\n",
      "0                                  making some lunch      0.0   \n",
      "1                        @Alexia You want his money.      0.0   \n",
      "2  @dizzyhrvy that crap took me forever to put to...      1.0   \n",
      "3  @jnaylor #kiwitweets Hey Jer! Since when did y...      0.0   \n",
      "4  Trying out &quot;Delicious Library 2&quot; wit...      0.0   \n",
      "\n",
      "                                      processed_text  \n",
      "0                                       making lunch  \n",
      "1                                  alexia want money  \n",
      "2  dizzyhrvy crap took forever put together im go...  \n",
      "3  jnaylor kiwitweets hey jer since start twittering  \n",
      "4  trying quotdelicious library quot mixed result...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Now apply it\n",
    "df['processed_text'] = df['Tweet'].apply(preprocess_text)\n",
    "\n",
    "# Drop missing\n",
    "df = df.dropna(subset=['processed_text', 'Suicide'])\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df38315f-f45b-4c35-8ef3-3667e2846df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sridher\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 110ms/step - accuracy: 0.6272 - loss: 0.6487 - val_accuracy: 0.8034 - val_loss: 0.4446\n",
      "Epoch 2/10\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.8816 - loss: 0.3021 - val_accuracy: 0.9120 - val_loss: 0.2349\n",
      "Epoch 3/10\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 0.9802 - loss: 0.0854 - val_accuracy: 0.9082 - val_loss: 0.2767\n",
      "Epoch 4/10\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.9869 - loss: 0.0470 - val_accuracy: 0.9045 - val_loss: 0.3113\n",
      "Epoch 5/10\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.9972 - loss: 0.0089 - val_accuracy: 0.8951 - val_loss: 0.3703\n",
      "Epoch 6/10\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9986 - loss: 0.0080 - val_accuracy: 0.9007 - val_loss: 0.3822\n",
      "Epoch 7/10\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9998 - loss: 0.0031 - val_accuracy: 0.8933 - val_loss: 0.4272\n",
      "Epoch 8/10\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.9977 - loss: 0.0118 - val_accuracy: 0.9157 - val_loss: 0.3933\n",
      "Epoch 9/10\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.9986 - loss: 0.0088 - val_accuracy: 0.8970 - val_loss: 0.4571\n",
      "Epoch 10/10\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.9971 - loss: 0.0043 - val_accuracy: 0.8689 - val_loss: 0.6158\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step  \n",
      "Accuracy: 0.8689138576779026\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89       330\n",
      "           1       0.79      0.89      0.84       204\n",
      "\n",
      "    accuracy                           0.87       534\n",
      "   macro avg       0.86      0.87      0.86       534\n",
      "weighted avg       0.88      0.87      0.87       534\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "The tweet indicates suicide ideation.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataframe df\n",
    "# df = pd.DataFrame({'processed_text': ['text1', 'text2'], 'Suicide': [0, 1]})\n",
    "\n",
    "# Prepare the text data for TF-IDF\n",
    "texts = df['processed_text'].tolist()\n",
    "\n",
    "# Prepare labels\n",
    "y = df['Suicide'].astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(texts, y, test_size=0.3, random_state=30)\n",
    "\n",
    "# Convert text data to TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_texts).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_texts).toarray()\n",
    "\n",
    "# Convert TF-IDF features to sequences for input to the neural network\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_texts)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_texts)\n",
    "\n",
    "# Pad sequences to the same length\n",
    "max_length = max(len(seq) for seq in X_train_seq + X_test_seq)\n",
    "print(max_length)\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Build the CNN+BiLSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=100, input_length=max_length))\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = (model.predict(X_test_padded) > 0.5).astype(\"int32\")\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "results = []\n",
    "results.append({\n",
    "    'Model': 'CNN+BiLSTM',\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': report['weighted avg']['precision'],\n",
    "    'Recall': report['weighted avg']['recall'],\n",
    "    'F1-Score': report['weighted avg']['f1-score']\n",
    "})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "#print(results_df)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Function to predict suicide ideation for a single tweet\n",
    "def predict_suicide_ideation(tweet):\n",
    "    # Preprocess the tweet\n",
    "    tweet_seq = tokenizer.texts_to_sequences([tweet])\n",
    "    tweet_padded = pad_sequences(tweet_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Predict using the trained model\n",
    "    prediction = (model.predict(tweet_padded) > 0.5).astype(\"int32\")\n",
    "\n",
    "    # Output result\n",
    "    if prediction[0][0] == 1:\n",
    "        print(\"The tweet indicates suicide ideation.\")\n",
    "    else:\n",
    "        print(\"The tweet does not indicate suicide ideation.\")\n",
    "\n",
    "# Example usage\n",
    "tweet = \"I want to die\"\n",
    "predict_suicide_ideation(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee10557f-f8b8-4ba5-bd3c-bf4dbf5564e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save max_length\n",
    "with open('max_length.pkl', 'wb') as f:\n",
    "    pickle.dump(max_length, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56c1f94e-f20c-413d-b87e-909ccf1302b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "# Save the tokenizer to a pickle file\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fa3d264-1a4c-473f-a2b5-8e8e1c13ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('config.json', 'w') as f:\n",
    "    json.dump({'max_length': max_length}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d238a2f9-6b5f-40f0-9a33-ff05fa905db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f370c42-5a32-42d6-aa04-6e6a2d5362cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
